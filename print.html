<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>spider-rs</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="index.html">Introduction</a></li><li class="chapter-item expanded affix "><li class="part-title">User Guide</li><li class="chapter-item expanded "><a href="getting-started.html"><strong aria-hidden="true">1.</strong> Getting started</a></li><li class="chapter-item expanded "><a href="simple.html"><strong aria-hidden="true">2.</strong> A simple example</a></li><li class="chapter-item expanded affix "><li class="part-title">Configuration</li><li class="chapter-item expanded "><a href="website.html"><strong aria-hidden="true">3.</strong> Website</a></li><li class="chapter-item expanded "><a href="page.html"><strong aria-hidden="true">4.</strong> Page</a></li><li class="chapter-item expanded "><a href="env.html"><strong aria-hidden="true">5.</strong> Environment</a></li><li class="chapter-item expanded affix "><li class="part-title">Usage</li><li class="chapter-item expanded "><a href="crawl.html"><strong aria-hidden="true">6.</strong> Crawl</a></li><li class="chapter-item expanded "><a href="scrape.html"><strong aria-hidden="true">7.</strong> Scrape</a></li><li class="chapter-item expanded "><a href="cron-job.html"><strong aria-hidden="true">8.</strong> Cron Job</a></li><li class="chapter-item expanded "><a href="storing-data.html"><strong aria-hidden="true">9.</strong> Storing Data</a></li><li class="chapter-item expanded affix "><li class="part-title">Benchmarks</li><li class="chapter-item expanded "><a href="benchmarks.html"><strong aria-hidden="true">10.</strong> Compare</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">spider-rs</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/spider-rs/spider-nodejs/tree/main/book" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p><code>Spider-RS</code> is the fastest web crawler and indexer written in Rust ported to Node.js.</p>
<ul>
<li>Concurrent</li>
<li>Streaming</li>
<li>Decentralization</li>
<li>Headless Chrome <a href="https://github.com/mattsse/chromiumoxide">Rendering</a></li>
<li>HTTP Proxies</li>
<li>Cron Jobs</li>
<li>Subscriptions</li>
<li>Blacklisting and Budgeting Depth</li>
<li>Written in <a href="https://www.rust-lang.org/">Rust</a> for speed, safety, and simplicity</li>
</ul>
<p>Spider powers some big tools and helps bring the crawling aspect to almost no downtime with the correct setup, view the <a href="https://github.com/spider-rs/spider">spider</a> project to learn more.</p>
<pre><code class="language-ts">import { Website } from "@spider-rs/spider-rs";

const website = new Website("https://choosealicense.com");

await website.crawl();

console.log(website.getLinks());
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="getting-started"><a class="header" href="#getting-started">Getting Started</a></h1>
<p>Make sure to have <a href="https://nodejs.org/en/download">node</a> installed v10 and higher.</p>
<p>Install the package with your favorite package manager.</p>
<pre><code class="language-sh">yarn add @spider-rs/spider-rs
# or
npm install @spider-rs/spider-rs
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="a-simple-example"><a class="header" href="#a-simple-example">A simple example</a></h1>
<p>We use the node-addon to port the Rust project over with napi to target node.js.</p>
<p>There are some performance drawbacks from the addon, even still the crawls are lightning fast and efficient.</p>
<h2 id="usage"><a class="header" href="#usage">Usage</a></h2>
<p>The examples below can help get started with spider.</p>
<h3 id="basic"><a class="header" href="#basic">Basic</a></h3>
<p>A basic example.</p>
<pre><code class="language-ts">import { Website } from "@spider-rs/spider-rs";

const website = new Website("https://choosealicense.com");

await website.crawl();
console.log(website.getLinks());
</code></pre>
<h3 id="events"><a class="header" href="#events">Events</a></h3>
<p>You can pass a function that could be async as param to <code>crawl</code> and <code>scrape</code>.</p>
<pre><code class="language-ts">import { Website, type NPage } from "@spider-rs/spider-rs";

const website = new Website("https://choosealicense.com");

const links: NPage[] = [];

const onPageEvent = async (err: Error | null, page: NPage) =&gt; {
  links.push(page);
};

await website.crawl(onPageEvent);
console.log(website.getLinks());
</code></pre>
<h3 id="selector"><a class="header" href="#selector">Selector</a></h3>
<p>The <code>title</code> method allows you to extract the title of the page.</p>
<pre><code class="language-ts">import { Website, pageTitle } from "@spider-rs/spider-rs";

const website = new Website("https://choosealicense.com");

const links = [];

const onPageEvent = async (err, page) =&gt; {
  links.push({ title: pageTitle(page), url: page.url });
};

// params in order event, background, and headless chrome
await website.crawl(onPageEvent);
</code></pre>
<h2 id="shortcut"><a class="header" href="#shortcut">Shortcut</a></h2>
<p>You can use the <code>crawl</code> shortcut method to collect contents quickly without configuration.</p>
<pre><code class="language-ts">import { crawl } from "@spider-rs/spider-rs";

const { links, pages } = await crawl("https://choosealicense.com");

console.log([links, pages]);
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="website"><a class="header" href="#website">Website</a></h1>
<p>The Website class is the foundations to the spider.</p>
<h2 id="builder-pattern"><a class="header" href="#builder-pattern">Builder pattern</a></h2>
<p>We use the builder pattern to configure the website for crawling.</p>
<p>*note: Replace <code>https://choosealicense.com</code> from the examples below with your website target URL.</p>
<pre><code class="language-ts">import { Website } from "@spider-rs/spider-rs";

const website = new Website("https://choosealicense.com");
</code></pre>
<h3 id="custom-headers"><a class="header" href="#custom-headers">Custom Headers</a></h3>
<p>Add custom HTTP headers to use when crawling/scraping.</p>
<pre><code class="language-ts">const website = new Website("https://choosealicense.com")
  .withHeaders({
    authorization: "somerandomjwt",
  })
  .build();
</code></pre>
<h3 id="blacklist"><a class="header" href="#blacklist">Blacklist</a></h3>
<p>Prevent crawling a set path, url, or pattern with Regex.</p>
<pre><code class="language-ts">const website = new Website("https://choosealicense.com")
  .withBlacklistUrl(["/blog", new RegExp("/books").source, "/resume"])
  .build();
</code></pre>
<h3 id="crons"><a class="header" href="#crons">Crons</a></h3>
<p>Setup a cron job that can run at any time in the background using cron-syntax.</p>
<pre><code class="language-ts">const website = new Website("https://choosealicense.com")
  .withCron("1/5 * * * * *")
  .build();
</code></pre>
<p>View the <a href="./cron-job.html">cron</a> section for details how to use the cron.</p>
<h3 id="budget"><a class="header" href="#budget">Budget</a></h3>
<p>Add a crawl budget that prevents crawling <code>x</code> amount of pages.</p>
<pre><code class="language-ts">const website = new Website("https://choosealicense.com")
  .withBudget({
    "*": 1,
  })
  .build();
</code></pre>
<h3 id="subdomains"><a class="header" href="#subdomains">Subdomains</a></h3>
<p>Include subdomains in request.</p>
<pre><code class="language-ts">const website = new Website("https://choosealicense.com")
  .withSubdomains(true)
  .build();
</code></pre>
<h3 id="tld"><a class="header" href="#tld">TLD</a></h3>
<p>Include TLDs in request.</p>
<pre><code class="language-ts">const website = new Website("https://choosealicense.com")
  .withTlds(true)
  .build();
</code></pre>
<h3 id="external-domains"><a class="header" href="#external-domains">External Domains</a></h3>
<p>Add external domains to include with the website.</p>
<pre><code class="language-ts">const website = new Website("https://choosealicense.com")
  .withExternalDomains(["https://www.myotherdomain.com"])
  .build();
</code></pre>
<h3 id="proxy"><a class="header" href="#proxy">Proxy</a></h3>
<p>Use a proxy to crawl a website.</p>
<pre><code class="language-ts">const website = new Website("https://choosealicense.com")
  .withProxies(["https://www.myproxy.com"])
  .build();
</code></pre>
<h3 id="delays"><a class="header" href="#delays">Delays</a></h3>
<p>Add delays between pages. Defaults to none.</p>
<pre><code class="language-ts">const website = new Website("https://choosealicense.com")
  .withDelays(200)
  .build();
</code></pre>
<h3 id="user-agent"><a class="header" href="#user-agent">User-Agent</a></h3>
<p>Use a custom User-Agent.</p>
<pre><code class="language-ts">const website = new Website("https://choosealicense.com")
  .withUserAgent("mybot/v1")
  .build();
</code></pre>
<h3 id="request-timeout"><a class="header" href="#request-timeout">Request Timeout</a></h3>
<p>Add a request timeout per page in miliseconds. Example shows 30 seconds.</p>
<pre><code class="language-ts">const website = new Website("https://choosealicense.com")
  .withRequestTimeout(30000)
  .build();
</code></pre>
<h3 id="respect-robots"><a class="header" href="#respect-robots">Respect Robots</a></h3>
<p>Respect the robots.txt file.</p>
<pre><code class="language-ts">const website = new Website("https://choosealicense.com")
  .withRespectRobotsTxt(true)
  .build();
</code></pre>
<h3 id="http2-prior-knowledge"><a class="header" href="#http2-prior-knowledge">Http2 Prior Knowledge</a></h3>
<p>Use http2 to connect if you know the website servers supports this.</p>
<pre><code class="language-ts">const website = new Website("https://choosealicense.com")
  .withHttp2PriorKnowledge(true)
  .build();
</code></pre>
<h3 id="chrome-network-interception"><a class="header" href="#chrome-network-interception">Chrome Network Interception</a></h3>
<p>Enable Network interception when using chrome to speed up request.</p>
<pre><code class="language-ts">const website = new Website("https://choosealicense.com")
  .withChromeIntercept(true, true)
  .build();
</code></pre>
<h3 id="redirect-limit"><a class="header" href="#redirect-limit">Redirect Limit</a></h3>
<p>Set the redirect limit for request.</p>
<pre><code class="language-ts">const website = new Website("https://choosealicense.com")
  .withRedirectLimit(2)
  .build();
</code></pre>
<h3 id="depth-limit"><a class="header" href="#depth-limit">Depth Limit</a></h3>
<p>Set the depth limit for the amount of forward pages.</p>
<pre><code class="language-ts">const website = new Website("https://choosealicense.com").withDepth(3).build();
</code></pre>
<h3 id="cache"><a class="header" href="#cache">Cache</a></h3>
<p>Enable HTTP caching, this useful when using the spider on a server.</p>
<pre><code class="language-ts">const website = new Website("https://choosealicense.com")
  .withCaching(true)
  .build();
</code></pre>
<h3 id="redirect-policy"><a class="header" href="#redirect-policy">Redirect Policy</a></h3>
<p>Set the redirect policy for request, either strict or loose(default). Strict only allows redirects that match the domain.</p>
<pre><code class="language-ts">const website = new Website("https://choosealicense.com")
  .withRedirectPolicy(true)
  .build();
</code></pre>
<h2 id="chaining"><a class="header" href="#chaining">Chaining</a></h2>
<p>You can chain all of the configs together for simple configuration.</p>
<pre><code class="language-ts">const website = new Website("https://choosealicense.com")
  .withSubdomains(true)
  .withTlds(true)
  .withUserAgent("mybot/v1")
  .withRespectRobotsTxt(true)
  .build();
</code></pre>
<h2 id="raw-content"><a class="header" href="#raw-content">Raw Content</a></h2>
<p>Set the second param of the website constructor to <code>true</code> to return content without UTF-8.
This will return <code>rawContent</code> and leave <code>content</code> when using subscriptions or the Page Object.</p>
<pre><code class="language-ts">const rawContent = true;
const website = new Website("https://choosealicense.com", rawContent);
await website.scrape();
</code></pre>
<h2 id="clearing-crawl-data"><a class="header" href="#clearing-crawl-data">Clearing Crawl Data</a></h2>
<p>Use <code>website.clear</code> to remove the links visited and page data or <code>website.drainLinks</code> to drain the links visited.</p>
<pre><code class="language-ts">const website = new Website("https://choosealicense.com");
await website.crawl();
// links found ["https://...", "..."]
console.log(website.getLinks());
website.clear();
// links will be empty
console.log(website.getLinks());
</code></pre>
<h2 id="storing-and-exporting-data"><a class="header" href="#storing-and-exporting-data">Storing and Exporting Data</a></h2>
<p>Collecting data to store can be done with <code>website.pushData()</code> and <code>website.exportJsonlData()</code>.</p>
<pre><code class="language-ts">const website = new Website("https://choosealicense.com");

const onPageEvent = (_err, page) =&gt; {
  website.pushData(page);
};

await website.crawl(onPageEvent);

// uncomment to read the data.
// console.log(website.readData());

// we only have one export method atm. Optional file path. All data by default goes to storage
await website.exportJsonlData("./storage/test.jsonl");
</code></pre>
<h2 id="stop-crawl"><a class="header" href="#stop-crawl">Stop crawl</a></h2>
<p>To stop a crawl you can use <code>website.stopCrawl(id)</code>, pass in the crawl id to stop a run or leave empty for all crawls to stop.</p>
<pre><code class="language-ts">const website = new Website("https://choosealicense.com");

const onPageEvent = (_err, page) =&gt; {
  console.log(page);
  // stop the concurrent crawl when 8 pages are found.
  if (website.size &gt;= 8) {
    website.stop();
  }
};

await website.crawl(onPageEvent);
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="page"><a class="header" href="#page">Page</a></h1>
<p>A single page on a website, useful if you need just the root url.</p>
<h2 id="new-page"><a class="header" href="#new-page">New Page</a></h2>
<p>Get a new page with content.</p>
<p>The first param is the url, followed by if subdomains should be included, and last to include TLD's in links.</p>
<p>Calling <code>page.fetch</code> is needed to get the content.</p>
<pre><code class="language-ts">import { Page } from "@spider-rs/spider-rs";

const page = new Page("https://choosealicense.com", false, false);
await page.fetch();
</code></pre>
<h2 id="page-links"><a class="header" href="#page-links">Page Links</a></h2>
<p>get all the links related to a page.</p>
<pre><code class="language-ts">const page = new Page("https://choosealicense.com", false, false);
await page.fetch();
const links = await page.getLinks();
console.log(links);
</code></pre>
<h2 id="page-html"><a class="header" href="#page-html">Page Html</a></h2>
<p>Get the markup for the page or HTML.</p>
<pre><code class="language-ts">const page = new Page("https://choosealicense.com", false, false);
await page.fetch();
const html = page.getHtml();
console.log(html);
</code></pre>
<h2 id="page-bytes"><a class="header" href="#page-bytes">Page Bytes</a></h2>
<p>Get the raw bytes of a page to store the files in a database.</p>
<pre><code class="language-ts">const page = new Page("https://choosealicense.com", false, false);
await page.fetch();
const bytes = page.getBytes();
console.log(bytes);
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="environment"><a class="header" href="#environment">Environment</a></h1>
<p>Env variables to adjust the project.</p>
<h2 id="chrome_url"><a class="header" href="#chrome_url">CHROME_URL</a></h2>
<p>You can set the chrome URL to connect remotely.</p>
<pre><code class="language-sh">CHROME_URL=http://localhost:9222
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="crawl"><a class="header" href="#crawl">Crawl</a></h1>
<p>Crawl a website concurrently.</p>
<pre><code class="language-ts">import { Website } from "@spider-rs/spider-rs";

// pass in the website url
const website = new Website("https://rsseau.fr");

await website.crawl();

// [ "https://rsseau.fr/blog", ...]
console.log(website.getLinks());
</code></pre>
<h2 id="async-event"><a class="header" href="#async-event">Async Event</a></h2>
<p>You can pass in a async function as the first param to the crawl function for realtime updates streamed.</p>
<pre><code class="language-ts">import { Website } from "@spider-rs/spider-rs";

const website = new Website("https://rsseau.fr");

const onPageEvent = (err, value) =&gt; {
  console.log(value);
};

await website.crawl(onPageEvent);
</code></pre>
<h2 id="background"><a class="header" href="#background">Background</a></h2>
<p>You can run the request in the background and receive events with the second param set to <code>true</code>.</p>
<pre><code class="language-ts">import { Website } from "@spider-rs/spider-rs";

const website = new Website("https://rsseau.fr");

const onPageEvent = (err, value) =&gt; {
  console.log(value);
};

await website.crawl(onPageEvent, true);
// this will run instantly as the crawl is in the background
</code></pre>
<h2 id="subscriptions"><a class="header" href="#subscriptions">Subscriptions</a></h2>
<p>You can setup many subscriptions to run events when a crawl happens.</p>
<pre><code class="language-ts">import { Website } from "@spider-rs/spider-rs";

const website = new Website("https://rsseau.fr");

const onPageEvent = (err, value) =&gt; {
  console.log(value);
};

const subscriptionID = website.subscribe(onPageEvent);

await website.crawl();

website.unsubscribe(subscriptionID);
// this will run instantly as the crawl is in the background
</code></pre>
<h2 id="headless-chrome"><a class="header" href="#headless-chrome">Headless Chrome</a></h2>
<p>Headless Chrome rendering can be done by setting the third param in <code>crawl</code> or <code>scrape</code> to <code>true</code>.
It will attempt to connect to chrome running remotely if the <code>CHROME_URL</code> env variable is set with chrome launching as a fallback. Using a remote connection with <code>CHROME_URL</code> will
drastically speed up runs.</p>
<pre><code class="language-ts">import { Website } from "@spider-rs/spider-rs";

const website = new Website("https://rsseau.fr");

const onPageEvent = (err, value) =&gt; {
  console.log(value);
};

// all params are optional. The third param determines headless rendering.
await website.crawl(onPageEvent, false, true);
// make sure to call unsubscribe when finished or else the instance is kept alive when events are setup.
website.unsubscribe();
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="scrape"><a class="header" href="#scrape">Scrape</a></h1>
<p>Scape a website and collect the resource data.</p>
<pre><code class="language-ts">import { Website } from "@spider-rs/spider-rs";

// pass in the website url
const website = new Website("https://rsseau.fr");

await website.scrape();

// [ { url: "https://rsseau.fr/blog", html: "&lt;html&gt;...&lt;/html&gt;"}, ...]
console.log(website.getPages());
</code></pre>
<h2 id="headless-chrome-1"><a class="header" href="#headless-chrome-1">Headless Chrome</a></h2>
<p>Headless Chrome rendering can be done by setting the third param in <code>crawl</code> or <code>scrape</code> to <code>true</code>.
It will attempt to connect to chrome running remotely if the <code>CHROME_URL</code> env variable is set with chrome launching as a fallback. Using a remote connection with <code>CHROME_URL</code> will
drastically speed up runs.</p>
<pre><code class="language-ts">import { Website } from "@spider-rs/spider-rs";

const website = new Website("https://rsseau.fr");

const onPageEvent = (err, value) =&gt; {
  console.log(value);
};

// all params are optional. The third param determines headless rendering.
await website.scrape(onPageEvent, false, true);
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cron-jobs"><a class="header" href="#cron-jobs">Cron Jobs</a></h1>
<p>Use a cron job that can run any time of day to gather website data.</p>
<pre><code class="language-ts">import { Website } from "@spider-rs/spider-rs";

const website = new Website("https://choosealicense.com")
  .withCron("1/5 * * * * *")
  .build();

// get the pages of the website when the cron runs streamed.
const onPageEvent = (err, value) =&gt; {
  console.log(value);
};

const handle = await website.runCron(onPageEvent);
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="storing-data"><a class="header" href="#storing-data">Storing Data</a></h1>
<p>Storing data can be done to collect the raw content for a website.</p>
<p>This allows you to upload and download the content without UTF-8 conversion. The property only appears when
setting the second param of the <code>Website</code> class constructor to true.</p>
<pre><code class="language-ts">const rawContent = true;

const links: Buffer[] = [];

const onPageEvent = (_err: Error | null, page: NPage) =&gt; {
  if (page.rawContent) {
    // we can download or store the content now to disk.
    links.push(page.rawContent);
  }
};

await website.crawl(onPageEvent);

const website = new Website("https://choosealicense.com", rawContent);
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="benchmarks"><a class="header" href="#benchmarks">Benchmarks</a></h1>
<p>Test url: <code>https://espn.com</code>
Mac M1 64gb 10-core CPU</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left"><code>libraries</code></th><th style="text-align: left"><code>pages</code></th><th style="text-align: left"><code>speed</code></th></tr></thead><tbody>
<tr><td style="text-align: left"><strong><code>spider(rust): crawl</code></strong></td><td style="text-align: left"><code>150,387</code></td><td style="text-align: left"><code>1m</code></td></tr>
<tr><td style="text-align: left"><strong><code>spider(nodejs): crawl</code></strong></td><td style="text-align: left"><code>150,387</code></td><td style="text-align: left"><code>153s</code></td></tr>
<tr><td style="text-align: left"><strong><code>spider(python): crawl</code></strong></td><td style="text-align: left"><code>150,387</code></td><td style="text-align: left"><code>186s</code></td></tr>
<tr><td style="text-align: left"><strong><code>scrapy(python): crawl</code></strong></td><td style="text-align: left"><code>49,598</code></td><td style="text-align: left"><code>1h</code></td></tr>
<tr><td style="text-align: left"><strong><code>crawlee(nodejs): crawl</code></strong></td><td style="text-align: left"><code>18,779</code></td><td style="text-align: left"><code>30m</code></td></tr>
</tbody></table>
</div>
<p>View the latest runs on <a href="https://github.com/spider-rs/spider-nodejs/actions/workflows/bench.yml">github</a>.</p>
<pre><code class="language-sh">-----------------------
Linux
2-core CPU
7 GB of RAM memory
-----------------------
</code></pre>
<p>Test url: <code>https://choosealicense.com</code> (small)
32 pages</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left"><code>libraries</code></th><th style="text-align: left"><code>speed</code></th></tr></thead><tbody>
<tr><td style="text-align: left"><strong><code>spider-rs: crawl 10 samples</code></strong></td><td style="text-align: left"><code>76ms</code></td></tr>
<tr><td style="text-align: left"><strong><code>crawlee: crawl 10 samples</code></strong></td><td style="text-align: left"><code>1s</code></td></tr>
</tbody></table>
</div>
<p>Test url: <code>https://rsseau.fr</code> (medium)
211 pages</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left"><code>libraries</code></th><th style="text-align: left"><code>speed</code></th></tr></thead><tbody>
<tr><td style="text-align: left"><strong><code>spider-rs: crawl 10 samples</code></strong></td><td style="text-align: left"><code>0.5s</code></td></tr>
<tr><td style="text-align: left"><strong><code>crawlee: crawl 10 samples</code></strong></td><td style="text-align: left"><code>72s</code></td></tr>
</tbody></table>
</div>
<pre><code class="language-sh">----------------------
mac Apple M1 Max
10-core CPU
64 GB of RAM memory
-----------------------
</code></pre>
<p>Test url: <code>https://choosealicense.com</code> (small)
32 pages</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left"><code>libraries</code></th><th style="text-align: left"><code>speed</code></th></tr></thead><tbody>
<tr><td style="text-align: left"><strong><code>spider-rs: crawl 10 samples</code></strong></td><td style="text-align: left"><code>286ms</code></td></tr>
<tr><td style="text-align: left"><strong><code>crawlee: crawl 10 samples</code></strong></td><td style="text-align: left"><code>1.7s</code></td></tr>
</tbody></table>
</div>
<p>Test url: <code>https://rsseau.fr</code> (medium)
211 pages</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left"><code>libraries</code></th><th style="text-align: left"><code>speed</code></th></tr></thead><tbody>
<tr><td style="text-align: left"><strong><code>spider-rs: crawl 10 samples</code></strong></td><td style="text-align: left"><code>2.5s</code></td></tr>
<tr><td style="text-align: left"><strong><code>crawlee: crawl 10 samples</code></strong></td><td style="text-align: left"><code>75s</code></td></tr>
</tbody></table>
</div>
<p>The performance scales the larger the website and if throttling is needed. Linux benchmarks are about 10x faster than macOS for spider-rs.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
